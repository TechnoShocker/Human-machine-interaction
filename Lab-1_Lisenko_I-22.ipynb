{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbI+mTL+W51G2OBGeCEb7x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Лабораторна робота № 1 \"Застосування логістичної регресії для аналізу тональності текстів\""],"metadata":{"id":"0VdunXuKLlmt"}},{"cell_type":"code","source":["import nltk\n","import numpy as np\n","import re\n","import string\n","from nltk.corpus import stopwords, twitter_samples\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer"],"metadata":{"id":"AvLQ9LIc_fYh","executionInfo":{"status":"ok","timestamp":1764342259769,"user_tz":-120,"elapsed":8070,"user":{"displayName":"Дмитро Лисенко","userId":"09417323770252187496"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["ЗАВДАННЯ 2: Функції попередньої обробки"],"metadata":{"id":"niCMixxtBsVT"}},{"cell_type":"code","source":["def process_tweet(tweet):\n","    \"\"\"\n","    Обробляє твіт: токенізація, видалення стоп-слів, стемінг.\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","\n","    # Видалення $GE, RT, посилань\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n","    # Видалення хештегів (#)\n","    tweet = re.sub(r'#', '', tweet)\n","\n","    # Токенізація\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and  # видалення стоп-слів\n","            word not in string.punctuation):   # видалення пунктуації\n","            stem_word = stemmer.stem(word)  # стемінг\n","            tweets_clean.append(stem_word)\n","\n","    return tweets_clean"],"metadata":{"id":"79AUKAgC_mky","executionInfo":{"status":"ok","timestamp":1764342277886,"user_tz":-120,"elapsed":8,"user":{"displayName":"Дмитро Лисенко","userId":"09417323770252187496"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["ЗАВДАННЯ 3: Побудова словника частотності"],"metadata":{"id":"whzygaBPBJZF"}},{"cell_type":"code","source":["def build_freqs(tweets, ys):\n","    \"\"\"\n","    Будує словник частот freqs[(word, label)] = count.\n","    \"\"\"\n","    # Конвертуємо np array в list\n","    yslist = np.squeeze(ys).tolist()\n","\n","    freqs = {}\n","    for y, tweet in zip(yslist, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, y)\n","            freqs[pair] = freqs.get(pair, 0) + 1\n","\n","    return freqs"],"metadata":{"id":"CpQ_uJDq_pki","executionInfo":{"status":"ok","timestamp":1764342288937,"user_tz":-120,"elapsed":43,"user":{"displayName":"Дмитро Лисенко","userId":"09417323770252187496"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["ЗАВДАННЯ 4: Реалізація логістичної регресії"],"metadata":{"id":"l4obG8CdBwwy"}},{"cell_type":"code","source":["def sigmoid(z):\n","    \"\"\"Сигмоїдна функція\"\"\"\n","    h = 1 / (1 + np.exp(-z))\n","    return h\n","\n","def gradientDescent(x, y, theta, alpha, num_iters):\n","    \"\"\"\n","    Реалізація градієнтного спуску.\n","    \"\"\"\n","    m = x.shape[0]\n","\n","    # Переконуємося, що y має правильну форму (m, 1)\n","    if y.ndim == 1:\n","        y = y.reshape(-1, 1)\n","\n","    for i in range(0, num_iters):\n","        # Обчислюємо гіпотезу\n","        z = np.dot(x, theta)\n","        h = sigmoid(z)\n","\n","        # Обчислюємо функцію втрат (log loss)\n","        epsilon = 1e-15  # Маленька константа для уникнення log(0)\n","        h_safe = np.clip(h, epsilon, 1 - epsilon) # Обмежуємо значення\n","        J = (-1/m) * np.sum(y * np.log(h_safe) + (1-y) * np.log(1 - h_safe))\n","\n","        # Обчислюємо градієнт\n","        grad = (1/m) * np.dot(x.T, (h - y))\n","\n","        # Оновлюємо параметри\n","        theta = theta - alpha * grad\n","\n","        if i % 100 == 0: # Друкуємо лог кожні 100 ітерацій\n","            print(f\"Ітерація {i}, Функція втрат J = {J:.8f}\")\n","\n","    return J, theta"],"metadata":{"id":"-vUsZX6z_wpT","executionInfo":{"status":"ok","timestamp":1764342306472,"user_tz":-120,"elapsed":15,"user":{"displayName":"Дмитро Лисенко","userId":"09417323770252187496"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["ЗАВДАННЯ 5: Функції вилучення ознак та передбачення"],"metadata":{"id":"csqmGoSJB0ej"}},{"cell_type":"code","source":["def extract_features(tweet, freqs):\n","    \"\"\"\n","    Формує вектор ознак (1, 3) для одного твіту.\n","    Ознаки: [bias, sum(pos_freqs), sum(neg_freqs)]\n","    \"\"\"\n","    word_l = process_tweet(tweet)\n","    x = np.zeros((1, 3))\n","    x[0,0] = 1  # Bias\n","\n","    for word in word_l:\n","        # Додаємо частоту слова з позитивного класу\n","        x[0,1] += freqs.get((word, 1.0), 0)\n","        # Додаємо частоту слова з негативного класу\n","        x[0,2] += freqs.get((word, 0.0), 0)\n","\n","    assert(x.shape == (1, 3))\n","    return x"],"metadata":{"id":"ZoqAp_J6_1gs","executionInfo":{"status":"ok","timestamp":1764342332587,"user_tz":-120,"elapsed":11,"user":{"displayName":"Дмитро Лисенко","userId":"09417323770252187496"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def predict_tweet(tweet, freqs, theta):\n","    \"\"\"\n","    Передбачає тональність одного твіту.\n","    \"\"\"\n","    x = extract_features(tweet, freqs)\n","    y_pred = sigmoid(np.dot(x, theta))\n","    return y_pred"],"metadata":{"id":"N1TI3iPI_5VV","executionInfo":{"status":"ok","timestamp":1764342353377,"user_tz":-120,"elapsed":42,"user":{"displayName":"Дмитро Лисенко","userId":"09417323770252187496"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def test_logistic_regression(test_x, test_y, freqs, theta):\n","    \"\"\"\n","    Тестує модель на тестовій вибірці та повертає точність.\n","    \"\"\"\n","    y_hat = []\n","\n","    for tweet in test_x:\n","        y_pred = predict_tweet(tweet, freqs, theta)\n","\n","        if y_pred > 0.5:\n","            y_hat.append(1.0)\n","        else:\n","            y_hat.append(0.0)\n","\n","    # Порівнюємо прогнози (y_hat) з реальними мітками (test_y)\n","    # np.squeeze() для коректного порівняння форм\n","    accuracy = np.mean(np.array(y_hat) == np.squeeze(test_y))\n","    return accuracy"],"metadata":{"id":"eVRDWvDZ_7ut","executionInfo":{"status":"ok","timestamp":1764342356429,"user_tz":-120,"elapsed":4,"user":{"displayName":"Дмитро Лисенко","userId":"09417323770252187496"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["ГОЛОВНИЙ БЛОК ВИКОНАННЯ"],"metadata":{"id":"iCP5ITItAT6n"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","    # 1. Завантаження NLTK даних\n","    nltk.download('twitter_samples', quiet=True)\n","    nltk.download('stopwords', quiet=True)\n","    nltk.download('punkt', quiet=True) # Потрібен для TweetTokenizer\n","\n","    # 2. (Завдання 1 & 2) Завантаження та розділення корпусу (як у описі)\n","    print(\"Завантаження та розділення корпусу 'twitter_samples'...\")\n","    all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","    all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n","\n","    # 4000 на навчання, 1000 на тест\n","    test_pos = all_positive_tweets[4000:]\n","    train_pos = all_positive_tweets[:4000]\n","    test_neg = all_negative_tweets[4000:]\n","    train_neg = all_negative_tweets[:4000]\n","\n","    train_x = train_pos + train_neg\n","    test_x = test_pos + test_neg\n","\n","    # Створюємо мітки: 1.0 для позитивних, 0.0 для негативних\n","    train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n","    test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n","    print(\"Дані підготовлено.\")\n","\n","    # 3. (Завдання 3) Побудова словника частотності\n","    print(\"\\nПобудова словника частот...\")\n","    freqs = build_freqs(train_x, train_y)\n","    print(f\"Словник частот побудовано. {len(freqs)} унікальних пар.\")\n","\n","    # 4. (Завдання 4 & 5) Навчання моделі\n","    print(\"\\n--- Початок навчання моделі ---\")\n","\n","    # Створення матриці ознак X (m, 3)\n","    X = np.zeros((len(train_x), 3))\n","    for i in range(len(train_x)):\n","        X[i, :]= extract_features(train_x[i], freqs)\n","\n","    Y = train_y # Y - це train_y\n","\n","    # Гіперпараметри\n","    alpha = 1e-9\n","    num_iters = 1500\n","\n","    # Навчання\n","    J, theta = gradientDescent(X, Y, np.zeros((3, 1)), alpha, num_iters)\n","\n","    print(\"\\n--- Навчання завершено ---\")\n","    print(f\"Фінальна помилка (Loss): {J:.8f}\")\n","    print(f\"Фінальні ваги (Theta): {[round(t, 8) for t in np.squeeze(theta)]}\")\n","\n","    # 5. (Завдання 6) Оцінка точності\n","    accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n","    print(f\"\\nТочність логістичної регресії на тестовій вибірці: {accuracy * 100:.2f}%\")\n","\n","    # 6. (Завдання 6 & 7) Тестування на власних прикладах\n","    print(\"\\n--- Тестування на власних прикладах ---\")\n","    my_tweet_pos = 'I am so happy and excited, this is the best day ever!'\n","    y_hat_pos = predict_tweet(my_tweet_pos, freqs, theta)\n","    print(f\"Твіт: '{my_tweet_pos}'\")\n","    print(f\"Прогноз: {y_hat_pos[0][0]:.4f} -> {'Позитивний' if y_hat_pos > 0.5 else 'Негативний'}\")\n","\n","    my_tweet_neg = 'This is a terrible and awful experience. I hate it.'\n","    y_hat_neg = predict_tweet(my_tweet_neg, freqs, theta)\n","    print(f\"\\nТвіт: '{my_tweet_neg}'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NR6iZj_d_9-m","executionInfo":{"status":"ok","timestamp":1764343838802,"user_tz":-120,"elapsed":10174,"user":{"displayName":"Дмитро Лисенко","userId":"09417323770252187496"}},"outputId":"cd20ba3a-3d82-4cf4-f43e-420147ca10eb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Завантаження та розділення корпусу 'twitter_samples'...\n","Дані підготовлено.\n","\n","Побудова словника частот...\n","Словник частот побудовано. 11397 унікальних пар.\n","\n","--- Початок навчання моделі ---\n","Ітерація 0, Функція втрат J = 0.69314718\n","Ітерація 100, Функція втрат J = 0.59538303\n","Ітерація 200, Функція втрат J = 0.52206432\n","Ітерація 300, Функція втрат J = 0.46560367\n","Ітерація 400, Функція втрат J = 0.42105697\n","Ітерація 500, Функція втрат J = 0.38517211\n","Ітерація 600, Функція втрат J = 0.35574646\n","Ітерація 700, Функція втрат J = 0.33124495\n","Ітерація 800, Функція втрат J = 0.31057008\n","Ітерація 900, Функція втрат J = 0.29291936\n","Ітерація 1000, Функція втрат J = 0.27769420\n","Ітерація 1100, Функція втрат J = 0.26444028\n","Ітерація 1200, Функція втрат J = 0.25280730\n","Ітерація 1300, Функція втрат J = 0.24252135\n","Ітерація 1400, Функція втрат J = 0.23336560\n","\n","--- Навчання завершено ---\n","Фінальна помилка (Loss): 0.22524410\n","Фінальні ваги (Theta): [np.float64(6e-08), np.float64(0.00053786), np.float64(-0.00055885)]\n","\n","Точність логістичної регресії на тестовій вибірці: 99.65%\n","\n","--- Тестування на власних прикладах ---\n","Твіт: 'I am so happy and excited, this is the best day ever!'\n","Прогноз: 0.5349 -> Позитивний\n","\n","Твіт: 'This is a terrible and awful experience. I hate it.'\n"]}]}]}