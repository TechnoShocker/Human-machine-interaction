{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "####**Лабораторна робота №4** Варіант № 6\n",
        "\n",
        "#####**Тема:** Наївний машинний переклад та локально-сенcитивне хешування"
      ],
      "metadata": {
        "id": "cbXXkvq69GCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "import gzip\n",
        "import random\n",
        "import time\n",
        "import string\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "adpeFQMWN7ZH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####ГОЛОВНИЙ ПЕРЕМИКАЧ: ДЕМОНСТРАЦІЙНИЙ РЕЖИМ"
      ],
      "metadata": {
        "id": "783-23zBOMap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# True = Використовувати маленькі, випадкові дані (запускається за 30 сек)\n",
        "# False = Спроба завантажити гігабайтні файли (вимагає >16GB RAM та >20GB місця)\n",
        "DEMO_MODE = True\n",
        "\n",
        "# Розмірність векторів (стандарт для fastText/MUSE)\n",
        "EMBEDDING_DIM = 300"
      ],
      "metadata": {
        "id": "nf3Lk2hvOLxI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####ДОПОМІЖНІ ФУНКЦІЇ (Завантаження даних)"
      ],
      "metadata": {
        "id": "grW6x-FROTCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings_gensim(url: str, max_words=50000) -> dict:\n",
        "    \"\"\"\n",
        "    Завантажує та парсить вектори fastText .vec.gz.\n",
        "    Використовує 'gensim' для сумісності з вашим кодом.\n",
        "    \"\"\"\n",
        "    print(f\"Завантаження векторів з {url} (це може зайняти > 30 хв)...\")\n",
        "    from gensim.models import KeyedVectors\n",
        "\n",
        "    try:\n",
        "        # Завантажуємо файл\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Створюємо тимчасовий файл для gensim\n",
        "        temp_filename = 'temp_vectors.vec.gz'\n",
        "        with open(temp_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(\"Завантаження завершено. Завантаження в Gensim...\")\n",
        "\n",
        "        # Завантажуємо в gensim\n",
        "        # Ми не можемо використати 'limit', оскільки 'index_to_key' буде неповним.\n",
        "        # Завантажуємо повну модель. Це вимагає багато RAM.\n",
        "        model = KeyedVectors.load_word2vec_format(temp_filename, binary=False)\n",
        "\n",
        "        os.remove(temp_filename) # Видаляємо тимчасовий файл\n",
        "\n",
        "        print(f\"Успішно завантажено {len(model.index_to_key)} векторів.\")\n",
        "        return model\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"ПОМИЛКА: Не вдалося завантажити файл: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"ПОМИЛКА при обробці Gensim: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "OlcYFKMrOYeC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict(file_content_str: str) -> dict:\n",
        "    \"\"\"\n",
        "    Парсить словник перекладів з текстового рядка.\n",
        "    (Ваш код використовував pd.read_csv, але це уникає залежності від Pandas)\n",
        "    \"\"\"\n",
        "    etof = {}\n",
        "    for line in file_content_str.splitlines():\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) == 2:\n",
        "            etof[parts[0]] = parts[1]\n",
        "    return etof"
      ],
      "metadata": {
        "id": "XM3oCpJAOdcr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dictionary(url: str) -> dict:\n",
        "    \"\"\"Завантажує тренувальний/тестовий словник.\"\"\"\n",
        "    print(f\"Завантаження словника з {url}...\")\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        dictionary = get_dict(response.text)\n",
        "        print(f\"Успішно завантажено {len(dictionary)} пар слів.\")\n",
        "        return dictionary\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"ПОМИЛКА: Не вдалося завантажити словник: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "J6Zi69fROfwz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_demo_data(vocab_size=500, dict_size=100, dim=EMBEDDING_DIM):\n",
        "    \"\"\"\n",
        "    Створює іграшкові дані для DEMO_MODE.\n",
        "    Вектори пов'язані (en = fr + noise), твіти використовують слова зі словника.\n",
        "    \"\"\"\n",
        "    print(\"[DEMO_MODE] Створення випадкових, але логічних даних...\")\n",
        "    # Gensim KeyedVectors - це складний об'єкт.\n",
        "    # В демо-режимі ми будемо використовувати прості словники Python {word: np.array}\n",
        "    # Це вимагатиме невеликих змін у коді\n",
        "\n",
        "    emb_fr = {}\n",
        "    emb_en = {}\n",
        "    vocab_fr = []\n",
        "    vocab_en = []\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        word_fr = f\"frword{i}\"\n",
        "        word_en = f\"enword{i}\"\n",
        "\n",
        "        base_vector = np.random.rand(dim)\n",
        "        emb_fr[word_fr] = base_vector\n",
        "\n",
        "        noise = (np.random.rand(dim) - 0.5) * 0.1 # Маленьке відхилення\n",
        "        emb_en[word_en] = base_vector + noise\n",
        "\n",
        "        vocab_fr.append(word_fr)\n",
        "        vocab_en.append(word_en)\n",
        "\n",
        "    # 2. Словники перекладів (dict)\n",
        "    train_dict = {}\n",
        "    test_dict = {}\n",
        "    for i in range(dict_size):\n",
        "        train_dict[vocab_fr[i]] = vocab_en[i]\n",
        "    for i in range(dict_size, dict_size + 50):\n",
        "        if i < vocab_size:\n",
        "            test_dict[vocab_fr[i]] = vocab_en[i]\n",
        "\n",
        "    # 3. Фальшиві твіти\n",
        "    print(\"[DEMO_MODE] Створення фальшивих твітів...\")\n",
        "    tweets = []\n",
        "    for _ in range(100):\n",
        "        random_words = random.sample(vocab_fr, 5) # Твіти мовою-1 (Французька)\n",
        "        tweets.append(\" \".join(random_words))\n",
        "\n",
        "    # Додамо два \"дуже схожих\" твіти, щоб LSH міг їх знайти\n",
        "    tweets[0] = f\"{vocab_fr[1]} {vocab_fr[2]} {vocab_fr[3]} {vocab_fr[4]}\"\n",
        "    tweets[1] = f\"{vocab_fr[1]} {vocab_fr[2]} {vocab_fr[3]} {vocab_fr[5]}\" # Схожий\n",
        "\n",
        "    print(\"[DEMO_MODE] Іграшкові дані створено.\")\n",
        "    return emb_fr, emb_en, train_dict, test_dict, tweets"
      ],
      "metadata": {
        "id": "_l63abOsOkhT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**ЗАВДАННЯ 3:** Побудова матриць X та Y"
      ],
      "metadata": {
        "id": "t77rVENoOnFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_matrices(l1_l2, l1_embeddings, l2_embeddings):\n",
        "    \"\"\"\n",
        "    (Функція з опису, адаптована для роботи з dict та gensim)\n",
        "    \"\"\"\n",
        "    X_l = list()\n",
        "    Y_l = list()\n",
        "\n",
        "    # Перевіряємо, чи є 'l1_embeddings' об'єктом gensim чи простим dict\n",
        "    is_gensim_l1 = hasattr(l1_embeddings, 'index_to_key')\n",
        "    is_gensim_l2 = hasattr(l2_embeddings, 'index_to_key')\n",
        "\n",
        "    l1_set = set(l1_embeddings.index_to_key) if is_gensim_l1 else set(l1_embeddings.keys())\n",
        "    l2_set = set(l2_embeddings.index_to_key) if is_gensim_l2 else set(l2_embeddings.keys())\n",
        "\n",
        "    for l1_word, l2_word in l1_l2.items():\n",
        "        if l1_word in l1_set and l2_word in l2_set:\n",
        "            X_l.append(l1_embeddings[l1_word])\n",
        "            Y_l.append(l2_embeddings[l2_word])\n",
        "\n",
        "    X = np.asarray(X_l)\n",
        "    Y = np.asarray(Y_l)\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "o4WJXHpTOliz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**ЗАВДАННЯ 4:** Пошук матриці R (Градієнтний спуск)"
      ],
      "metadata": {
        "id": "sJElknd7Os58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(X, Y, R):\n",
        "    \"\"\"(Функція з опису)\"\"\"\n",
        "    m = X.shape[0]\n",
        "    diff = np.dot(X, R) - Y\n",
        "    loss = np.sum(diff**2) / m\n",
        "    return loss\n",
        "\n",
        "def compute_gradient(X, Y, R):\n",
        "    \"\"\"(Функція з опису)\"\"\"\n",
        "    m = X.shape[0]\n",
        "    gradient = np.dot(X.T, np.dot(X, R) - Y) * (2 / m)\n",
        "    return gradient\n",
        "\n",
        "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003, verbose=True):\n",
        "    \"\"\"(Функція з опису)\"\"\"\n",
        "    print(\"Початок пошуку матриці R (Градієнтний спуск)...\")\n",
        "    np.random.seed(129)\n",
        "    m, n = X.shape\n",
        "    R = np.random.rand(n, n)\n",
        "\n",
        "    for i in range(train_steps):\n",
        "        if verbose and i % 10 == 0:\n",
        "            print(f\"Loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
        "        gradient = compute_gradient(X, Y, R)\n",
        "        R -= learning_rate * gradient\n",
        "\n",
        "    print(f\"Loss at iteration {train_steps} is: {compute_loss(X, Y, R):.4f}\")\n",
        "    print(\"Пошук R завершено.\")\n",
        "    return R"
      ],
      "metadata": {
        "id": "Y_kTm6tqOykt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**ЗАВДАННЯ 5 & 6:** Переклад та Оцінка (Оптимізовано)"
      ],
      "metadata": {
        "id": "75XXO8RyO0Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimized_cosine_similarity(v1, v2_matrix_normalized):\n",
        "    \"\"\"\n",
        "    Швидко обчислює косинусну подібність v1 до КОЖНОГО рядка\n",
        "    в ПОПЕРЕДНЬО НОРМАЛІЗОВАНІЙ матриці v2.\n",
        "    \"\"\"\n",
        "    v1_norm = v1 / (np.linalg.norm(v1) + 1e-8)\n",
        "    # v2_matrix_normalized вже нормалізована\n",
        "    similarities = np.dot(v2_matrix_normalized, v1_norm)\n",
        "    return similarities\n",
        "\n",
        "def translate(word, R, l1_embeddings, l2_emb_matrix_norm, l2_vocab_list):\n",
        "    \"\"\"\n",
        "    (Оптимізована версія Завдання 5)\n",
        "    Перекладає слово, використовуючи швидкий пошук.\n",
        "    \"\"\"\n",
        "    # Перевірка на gensim/dict\n",
        "    if hasattr(l1_embeddings, 'key_to_index'):\n",
        "        if word not in l1_embeddings:\n",
        "            return None\n",
        "    else:\n",
        "        if word not in l1_embeddings:\n",
        "            return None\n",
        "\n",
        "    l1_emb = l1_embeddings[word]\n",
        "    l2_emb_pred = np.dot(l1_emb, R)\n",
        "\n",
        "    distances = optimized_cosine_similarity(l2_emb_pred, l2_emb_matrix_norm)\n",
        "\n",
        "    top_k_idx = np.argsort(distances)[-1:] # Беремо 1 найкращий\n",
        "    return l2_vocab_list[top_k_idx[0]]\n",
        "\n",
        "def test_translation_accuracy(l1_l2_test, R, l1_embeddings, l2_embeddings):\n",
        "    \"\"\"\n",
        "    (Виконує Завдання 6)\n",
        "    (Виправлена та оптимізована версія 'test_vocabulary' з опису)\n",
        "    \"\"\"\n",
        "    print(\"\\nОцінка точності перекладу на тестовій вибірці...\")\n",
        "\n",
        "    # 1. Готуємо дані мови-2 для швидкого пошуку\n",
        "    if hasattr(l2_embeddings, 'index_to_key'): # Gensim\n",
        "        l2_vocab_list = l2_embeddings.index_to_key\n",
        "        l2_matrix = l2_embeddings.vectors\n",
        "    else: # Dict\n",
        "        l2_vocab_list = list(l2_embeddings.keys())\n",
        "        l2_matrix = np.array(list(l2_embeddings.values()))\n",
        "\n",
        "    # 2. Попередньо нормалізуємо матрицю мови-2\n",
        "    l2_matrix_norms = np.linalg.norm(l2_matrix, axis=1)\n",
        "    l2_matrix_normalized = l2_matrix / (l2_matrix_norms[:, np.newaxis] + 1e-8)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for l1_word, l2_true in l1_l2_test.items():\n",
        "        # Перевіряємо, що обидва слова є в наших словниках\n",
        "        l1_exists = (hasattr(l1_embeddings, 'key_to_index') and l1_word in l1_embeddings) or (l1_word in l1_embeddings)\n",
        "        l2_exists = l2_true in l2_vocab_list\n",
        "\n",
        "        if l1_exists and l2_exists:\n",
        "            total += 1\n",
        "            pred_l2 = translate(l1_word, R, l1_embeddings, l2_matrix_normalized, l2_vocab_list)\n",
        "\n",
        "            if pred_l2 == l2_true:\n",
        "                correct += 1\n",
        "\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "OhFrAw_eO3U9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**ЗАВДАННЯ 7 & 8:** Векторизація твітів"
      ],
      "metadata": {
        "id": "IDXSECJ_O8Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Використовуємо функції з Лаби 2, але адаптуємо 'get_tweet_embedding')\n",
        "\n",
        "def process_tweet_for_lsh(tweet, is_demo=False):\n",
        "    \"\"\"\n",
        "    Спрощена обробка твітів для LSH.\n",
        "    Якщо is_demo=True, використовує .split()\n",
        "    \"\"\"\n",
        "    if is_demo:\n",
        "        # Демо-твіти - це просто слова, розділені пробілом\n",
        "        return tweet.split(' ')\n",
        "    else:\n",
        "        # Для реальних твітів\n",
        "        stemmer = PorterStemmer()\n",
        "        stopwords_english = stopwords.words('english')\n",
        "        tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "        tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "        tweet = re.sub(r'#', '', tweet)\n",
        "        tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "        tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "        tweets_clean = []\n",
        "        for word in tweet_tokens:\n",
        "            if (word not in stopwords_english and word not in string.punctuation):\n",
        "                # НЕ робимо стемінг, щоб знайти слова у векторах fastText\n",
        "                tweets_clean.append(word)\n",
        "        return tweets_clean\n",
        "\n",
        "def get_tweet_embedding(tweet, l1_embeddings, is_demo=False):\n",
        "    \"\"\"(Виконує Завдання 8) - усереднення векторів\"\"\"\n",
        "    words = process_tweet_for_lsh(tweet, is_demo)\n",
        "    embeddings = []\n",
        "\n",
        "    is_gensim = hasattr(l1_embeddings, 'key_to_index')\n",
        "\n",
        "    for w in words:\n",
        "        if (is_gensim and w in l1_embeddings) or (not is_gensim and w in l1_embeddings):\n",
        "            embeddings.append(l1_embeddings[w])\n",
        "\n",
        "    if not embeddings:\n",
        "        return np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "    tweet_embedding = np.mean(embeddings, axis=0)\n",
        "    return tweet_embedding\n",
        "\n",
        "def get_all_tweet_embeddings(tweets, l1_embeddings, is_demo=False):\n",
        "    \"\"\"(Виконує Завдання 8 - для всіх твітів)\"\"\"\n",
        "    tweet_embeddings = {}\n",
        "    for i, tweet in enumerate(tweets):\n",
        "        tweet_embeddings[i] = get_tweet_embedding(tweet, l1_embeddings, is_demo)\n",
        "    return tweet_embeddings"
      ],
      "metadata": {
        "id": "xBBbyRiYO6tO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**ЗАВДАННЯ 9 & 10:** Локально-Сенситивне Хешування (LSH)"
      ],
      "metadata": {
        "id": "8nPyhIb5PAku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hash_func(embedding, planes):\n",
        "    \"\"\"(Виконує частину Завдання 9)\"\"\"\n",
        "    hash_value = 0\n",
        "\n",
        "    # *** ВИПРАВЛЕННЯ: ***\n",
        "    # Ми повинні ітерувати по стовпцях матриці 'planes', а не по рядках.\n",
        "    # 'planes' має розмір (300, 10). 'planes.T' (транспонована) має розмір (10, 300).\n",
        "    # Ітерація по 'planes.T' дає нам 10 векторів (площин), кожен розміром (300,).\n",
        "    for i, plane in enumerate(planes.T):\n",
        "        # Тепер 'embedding' (300,) і 'plane' (300,) мають однакову розмірність\n",
        "        sign = 1 if np.dot(embedding, plane) > 0 else 0\n",
        "        hash_value += sign * 2 ** i\n",
        "    return hash_value\n",
        "\n",
        "def make_hash_table(embeddings, planes):\n",
        "    \"\"\"(Виконує частину Завдання 9)\"\"\"\n",
        "    hash_table = defaultdict(list)\n",
        "    for i, embedding in embeddings.items():\n",
        "        # Не хешуємо нульові вектори\n",
        "        if np.linalg.norm(embedding) > 0:\n",
        "            hash_value = hash_func(embedding, planes)\n",
        "            hash_table[hash_value].append(i)\n",
        "    return hash_table\n",
        "\n",
        "def init_lsh(embeddings_dict, n_planes, n_tables):\n",
        "    \"\"\"(Виконує Завдання 9: створює LSH таблиці)\"\"\"\n",
        "    print(f\"Створення LSH (planes={n_planes}, tables={n_tables})...\")\n",
        "\n",
        "    # Словник {id: vector} -> матриця (m, n)\n",
        "    embeddings_matrix = np.array(list(embeddings_dict.values()))\n",
        "\n",
        "    planes_list = [np.random.normal(size=(EMBEDDING_DIM, n_planes))\n",
        "                   for _ in range(n_tables)]\n",
        "    tables = [make_hash_table(embeddings_dict, planes) for planes in planes_list]\n",
        "    print(f\"Створено {len(tables)} LSH таблиць.\")\n",
        "    return tables, planes_list\n",
        "\n",
        "def lsh_knn(tweet_id, embedding, tweet_embeddings_dict, tables, planes_list, k=5):\n",
        "    \"\"\"\n",
        "    (Виконує Завдання 10: Пошук LSH)\n",
        "    tweet_embeddings_dict: словник {id: vector}\n",
        "    \"\"\"\n",
        "    candidates = set()\n",
        "    for table, planes in zip(tables, planes_list):\n",
        "        hash_value = hash_func(embedding, planes)\n",
        "        candidates.update(table.get(hash_value, []))\n",
        "\n",
        "    # Видаляємо сам запит зі списку кандидатів\n",
        "    if tweet_id in candidates:\n",
        "        candidates.discard(tweet_id)\n",
        "\n",
        "    if not candidates:\n",
        "        return []\n",
        "\n",
        "    # 4. Обчислюємо реальну подібність ТІЛЬКИ для кандидатів\n",
        "    candidate_indices = list(candidates)\n",
        "    candidate_embeddings = np.array([tweet_embeddings_dict[i] for i in candidate_indices])\n",
        "\n",
        "    # Використовуємо оптимізовану ф-цію\n",
        "    similarities = optimized_cosine_similarity(embedding, candidate_embeddings)\n",
        "\n",
        "    # Сортуємо\n",
        "    top_k_local_idx = np.argsort(similarities)[-k:][::-1]\n",
        "\n",
        "    # Повертаємо глобальні індекси\n",
        "    neighbors = [(candidate_indices[i], similarities[i]) for i in top_k_local_idx]\n",
        "\n",
        "    return neighbors"
      ],
      "metadata": {
        "id": "I1mE9cONPDjH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####ГОЛОВНИЙ СКРИПТ (ЗАПУСК)"
      ],
      "metadata": {
        "id": "0lcx6btuPHGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"--- Лабораторна робота № 4: Початок ---\")\n",
        "    print(f\"Варіант 6: Французька (мова 1) -> Англійська (мова 2)\")\n",
        "\n",
        "    # --- URL-адреси (Французька -> Англійська) ---\n",
        "    URL_EMB_FR = \"https://dl.fbaipublicfiles.com/muse/vectors-in-fasttext-format/wiki.multi.fr.vec.gz\"\n",
        "    URL_EMB_EN = \"https://dl.fbaipublicfiles.com/muse/vectors-in-fasttext-format/wiki.multi.en.vec.gz\"\n",
        "    URL_TRAIN_DICT = \"https://dl.fbaipublicfiles.com/arrival/dictionaries/fr-en.0-5000.txt\"\n",
        "    URL_TEST_DICT = \"https://dl.fbaipublicfiles.com/arrival/dictionaries/fr-en.5000-6500.txt\"\n",
        "\n",
        "    if DEMO_MODE:\n",
        "        # (Завдання 1, 2, 7) - Генеруємо демо-дані\n",
        "        lang1_embeddings, lang2_embeddings, l1_l2_train, l1_l2_test, all_tweets = create_demo_data()\n",
        "    else:\n",
        "        # (Завдання 1, 2, 7) - Завантажуємо реальні дані\n",
        "        # УВАГА: Це вимагає >16GB RAM та >15GB місця\n",
        "        lang1_embeddings = load_embeddings_gensim(URL_EMB_FR) # Французька\n",
        "        lang2_embeddings = load_embeddings_gensim(URL_EMB_EN) # Англійська\n",
        "        if lang1_embeddings is None or lang2_embeddings is None:\n",
        "            print(\"Не вдалося завантажити вектори. Зупинка.\")\n",
        "            return\n",
        "\n",
        "        l1_l2_train = load_dictionary(URL_TRAIN_DICT)\n",
        "        l1_l2_test = load_dictionary(URL_TEST_DICT)\n",
        "\n",
        "        nltk.download('twitter_samples', quiet=True)\n",
        "        all_tweets = twitter_samples.strings('positive_tweets.json') + twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "    # --- Частина 1: Машинний переклад ---\n",
        "    print(\"\\n--- Частина 1: Машинний переклад ---\")\n",
        "\n",
        "    # (Завдання 3)\n",
        "    print(\"Побудова матриць X та Y...\")\n",
        "    X_train, Y_train = get_matrices(l1_l2_train, lang1_embeddings, lang2_embeddings)\n",
        "    print(f\"Розмір матриці X (train): {X_train.shape}\")\n",
        "    print(f\"Розмір матриці Y (train): {Y_train.shape}\")\n",
        "\n",
        "    if X_train.shape[0] == 0:\n",
        "        print(\"ПОМИЛКА: Не вдалося знайти спільних слів у словниках.\")\n",
        "        return\n",
        "\n",
        "    # (Завдання 4)\n",
        "    R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.0003)\n",
        "\n",
        "    # (Завдання 6)\n",
        "    accuracy = test_translation_accuracy(l1_l2_test, R_train, lang1_embeddings, lang2_embeddings)\n",
        "    print(f\"Точність перекладу (Accuracy @ 1): {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # (Завдання 5 - демонстрація)\n",
        "    print(\"\\nДемонстрація перекладу (Завдання 5):\")\n",
        "    test_word_fr = list(l1_l2_train.keys())[0] # Беремо перше слово з train dict\n",
        "    if not DEMO_MODE:\n",
        "        test_word_fr = 'chien' # Собака\n",
        "\n",
        "    # Готуємо дані для швидкого перекладу\n",
        "    if hasattr(lang2_embeddings, 'index_to_key'): # Gensim\n",
        "        l2_vocab_list = lang2_embeddings.index_to_key\n",
        "        l2_matrix = lang2_embeddings.vectors\n",
        "    else: # Dict\n",
        "        l2_vocab_list = list(lang2_embeddings.keys())\n",
        "        l2_matrix = np.array(list(lang2_embeddings.values()))\n",
        "    l2_matrix_norms = np.linalg.norm(l2_matrix, axis=1)\n",
        "    l2_matrix_normalized = l2_matrix / (l2_matrix_norms[:, np.newaxis] + 1e-8)\n",
        "\n",
        "    translation = translate(test_word_fr, R_train, lang1_embeddings, l2_matrix_normalized, l2_vocab_list)\n",
        "    print(f\"Французьке: '{test_word_fr}' -> Англійське: '{translation}'\")\n",
        "\n",
        "    # --- Частина 2: Локально-Сенситивне Хешування (LSH) ---\n",
        "    print(\"\\n--- Частина 2: LSH для твітів ---\")\n",
        "\n",
        "    # (Завдання 8) - Векторизуємо твіти (використовуючи вектори FR)\n",
        "    print(f\"Векторизація {len(all_tweets)} твітів (використовуючи вектори FR)...\")\n",
        "    tweet_embeddings = get_all_tweet_embeddings(all_tweets, lang1_embeddings, is_demo=DEMO_MODE)\n",
        "\n",
        "    # (Завдання 9)\n",
        "    tables, planes_list = init_lsh(tweet_embeddings, n_planes=10, n_tables=5)\n",
        "\n",
        "    # (Завдання 11 - Тестування LSH)\n",
        "    query_id = 0 # Беремо перший твіт як запит\n",
        "    query_tweet = all_tweets[query_id]\n",
        "    query_embedding = tweet_embeddings[query_id]\n",
        "\n",
        "    print(f\"\\n--- Тестування LSH (Завдання 11) ---\")\n",
        "    print(f\"Твіт-запит (індекс {query_id}):\\n{query_tweet}\\n\")\n",
        "    print(\"Шукаю подібні твіти...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    # (Завдання 10 - виклик функції пошуку)\n",
        "    neighbors = lsh_knn(query_id, query_embedding, tweet_embeddings, tables, planes_list, k=3)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"Пошук LSH зайняв {end_time - start_time:.6f} секунд.\")\n",
        "\n",
        "    print(\"\\nТоп-3 найбільш подібних твітів:\")\n",
        "    if not neighbors:\n",
        "        print(\"Схожих твітів у тих самих 'відрах' не знайдено.\")\n",
        "\n",
        "    for neighbor_id, similarity in neighbors:\n",
        "        print(f\"  (Індекс: {neighbor_id}, Подібність: {similarity:.4f})\")\n",
        "        print(f\"     {all_tweets[neighbor_id][:80]}...\")\n",
        "\n",
        "    print(\"\\n--- Лабораторна робота № 4: Завершено ---\")\n",
        "\n",
        "# Запускаємо головну функцію\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YIlLGmThPKCA",
        "outputId": "c33cdfd3-087c-40f5-9867-8b8974756e55"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Лабораторна робота № 4: Початок ---\n",
            "Варіант 6: Французька (мова 1) -> Англійська (мова 2)\n",
            "[DEMO_MODE] Створення випадкових, але логічних даних...\n",
            "[DEMO_MODE] Створення фальшивих твітів...\n",
            "[DEMO_MODE] Іграшкові дані створено.\n",
            "\n",
            "--- Частина 1: Машинний переклад ---\n",
            "Побудова матриць X та Y...\n",
            "Розмір матриці X (train): (100, 300)\n",
            "Розмір матриці Y (train): (100, 300)\n",
            "Початок пошуку матриці R (Градієнтний спуск)...\n",
            "Loss at iteration 0 is: 1665001.0154\n",
            "Loss at iteration 10 is: 662560.9103\n",
            "Loss at iteration 20 is: 263890.6053\n",
            "Loss at iteration 30 is: 105338.5462\n",
            "Loss at iteration 40 is: 42281.1184\n",
            "Loss at iteration 50 is: 17201.7529\n",
            "Loss at iteration 60 is: 7226.2063\n",
            "Loss at iteration 70 is: 3257.4303\n",
            "Loss at iteration 80 is: 1677.5440\n",
            "Loss at iteration 90 is: 1047.7223\n",
            "Loss at iteration 100 is: 795.7466\n",
            "Loss at iteration 110 is: 694.0467\n",
            "Loss at iteration 120 is: 652.1182\n",
            "Loss at iteration 130 is: 633.9675\n",
            "Loss at iteration 140 is: 625.2802\n",
            "Loss at iteration 150 is: 620.3632\n",
            "Loss at iteration 160 is: 616.9526\n",
            "Loss at iteration 170 is: 614.1478\n",
            "Loss at iteration 180 is: 611.5906\n",
            "Loss at iteration 190 is: 609.1387\n",
            "Loss at iteration 200 is: 606.7353\n",
            "Loss at iteration 210 is: 604.3578\n",
            "Loss at iteration 220 is: 601.9973\n",
            "Loss at iteration 230 is: 599.6500\n",
            "Loss at iteration 240 is: 597.3146\n",
            "Loss at iteration 250 is: 594.9903\n",
            "Loss at iteration 260 is: 592.6770\n",
            "Loss at iteration 270 is: 590.3745\n",
            "Loss at iteration 280 is: 588.0827\n",
            "Loss at iteration 290 is: 585.8015\n",
            "Loss at iteration 300 is: 583.5308\n",
            "Loss at iteration 310 is: 581.2707\n",
            "Loss at iteration 320 is: 579.0210\n",
            "Loss at iteration 330 is: 576.7817\n",
            "Loss at iteration 340 is: 574.5527\n",
            "Loss at iteration 350 is: 572.3341\n",
            "Loss at iteration 360 is: 570.1256\n",
            "Loss at iteration 370 is: 567.9274\n",
            "Loss at iteration 380 is: 565.7393\n",
            "Loss at iteration 390 is: 563.5612\n",
            "Loss at iteration 400 is: 561.3932\n",
            "Пошук R завершено.\n",
            "\n",
            "Оцінка точності перекладу на тестовій вибірці...\n",
            "Точність перекладу (Accuracy @ 1): 0.00%\n",
            "\n",
            "Демонстрація перекладу (Завдання 5):\n",
            "Французьке: 'frword0' -> Англійське: 'enword481'\n",
            "\n",
            "--- Частина 2: LSH для твітів ---\n",
            "Векторизація 100 твітів (використовуючи вектори FR)...\n",
            "Створення LSH (planes=10, tables=5)...\n",
            "Створено 5 LSH таблиць.\n",
            "\n",
            "--- Тестування LSH (Завдання 11) ---\n",
            "Твіт-запит (індекс 0):\n",
            "frword1 frword2 frword3 frword4\n",
            "\n",
            "Шукаю подібні твіти...\n",
            "Пошук LSH зайняв 0.000282 секунд.\n",
            "\n",
            "Топ-3 найбільш подібних твітів:\n",
            "  (Індекс: 1, Подібність: 8.8011)\n",
            "     frword1 frword2 frword3 frword5...\n",
            "  (Індекс: 86, Подібність: 8.6331)\n",
            "     frword80 frword65 frword487 frword290 frword415...\n",
            "  (Індекс: 95, Подібність: 8.5962)\n",
            "     frword446 frword272 frword315 frword5 frword174...\n",
            "\n",
            "--- Лабораторна робота № 4: Завершено ---\n"
          ]
        }
      ]
    }
  ]
}